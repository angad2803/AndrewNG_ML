# ğŸ“˜ Regularization in Machine Learning â€” Summary

Regularization is a technique used to reduce **overfitting** in machine learning models by penalizing large weights.

---

## ğŸ”‘ Key Concepts

### 1. **Problem of Overfitting**

- High-degree polynomials (e.g. xÂ³, xâ´) can fit training data too well (wiggly curves).
- These models perform poorly on new/unseen data (poor generalization).

---

### 2. **Solution: Regularization**

- Shrinks large weight values **wâ‚ to wâ‚™** toward zero.
- Simpler models generalize better.
- Keeps all features, unlike feature selection.

---

### 3. **Modified Cost Function**

#### Original (Linear Regression):

\[
J(w, b) = \frac{1}{2m} \sum (f(x^{(i)}) - y^{(i)})^2
\]

#### Regularized:

\[
J*{reg}(w, b) = \frac{1}{2m} \sum (f(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m} \sum*{j=1}^{n} w_j^2
\]

- The second term is the **regularization term**.
- \(\lambda\) (lambda) is the **regularization parameter**.

---

### 4. **Effect of Lambda (Î»)**

| Î» Value   | Effect                                     |
| --------- | ------------------------------------------ |
| Î» = 0     | No regularization â†’ Overfitting            |
| Î» â†’ âˆ     | Very strong regularization â†’ Underfitting  |
| Optimal Î» | Balance between underfitting & overfitting |

---

### 5. **Practical Notes**

- Usually **do not regularize** the bias term (b).
- Regularization is commonly used in **linear**, **logistic**, and **neural networks**.
- Helps maintain all features, but limits their influence.

---

## âœ… Summary

Regularization helps prevent overfitting by penalizing large weights through an extra term in the cost function. Choosing a good Î» is key to balancing model complexity and generalization.

# ğŸ§  Regularized Linear Regression: Gradient Descent

## ğŸ“˜ Cost Function (with Regularization)

```
J(w, b) = (1/2m) * Î£ [f(xá¶¦) - yá¶¦]Â² + (Î»/2m) * Î£ [wâ±¼Â²]
```

- First term: Squared error (fit to data)
- Second term: Regularization (shrinks parameters)
- Î» (lambda): Regularization parameter
- m: Number of training examples

---

## ğŸ” Gradient Descent Updates (Regularized)

### For wâ±¼ (j = 1 to n)

```
wâ±¼ := wâ±¼ - Î± * [ (1/m) * Î£(f(xá¶¦) - yá¶¦) * xâ±¼á¶¦ + (Î»/m) * wâ±¼ ]
```

### For b (not regularized)

```
b := b - Î± * (1/m) * Î£(f(xá¶¦) - yá¶¦)
```

---

## ğŸ” Interpretation

- Regularization adds `+ (Î»/m) * wâ±¼` to the gradient.
- Update rule becomes:

```
wâ±¼ := wâ±¼ * (1 - Î±Î»/m) - Î± * usual_gradient
```

â†’ This shrinks `wâ±¼` slightly every iteration.

---

## ğŸ’¡ Why It Works

- Penalizing large weights reduces overfitting.
- Keeps model simpler â†’ better generalization.
- Î» too large â†’ underfitting  
  Î» = 0 â†’ overfitting  
  Choose **just right** via validation.

---

## ğŸ§® Optional Derivative Breakdown

```
âˆ‚J/âˆ‚wâ±¼ = (1/m) * Î£[(f(xá¶¦) - yá¶¦) * xâ±¼á¶¦] + (Î»/m) * wâ±¼
```

---

## âœ… Summary

- Regularization shrinks `wâ±¼` on each iteration.
- Prevents overfitting in models with many features.
- Small change to gradient descent â†’ big improvement.

# Regularized Logistic Regression Summary

## ğŸ¯ Goal

Implement logistic regression with **regularization** to prevent **overfitting**, especially when working with high-order polynomial features or many input features.

---

## ğŸ§  Background

Just like linear regression, logistic regression can **overfit** if trained on a complex model with many features. Regularization helps prevent this by **penalizing large weights (w)**.

---

## ğŸ“‰ Cost Function with Regularization

### Original Cost Function (Logistic Regression):

$$
J(w, b) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(f^{(i)}) + (1 - y^{(i)}) \log(1 - f^{(i)}) \right]
$$

where \( f^{(i)} = \sigma(z^{(i)}) = \sigma(w^T x^{(i)} + b) \)

### Regularized Cost Function:

$$
J(w, b) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(f^{(i)}) + (1 - y^{(i)}) \log(1 - f^{(i)}) \right] + \frac{\lambda}{2m} \sum_{j=1}^{n} w_j^2
$$

- \( \lambda \): Regularization parameter.
- Note: **Only w is regularized**, not b.

---

## ğŸ” Gradient Descent Update Rules

### Derivatives with Regularization:

- For \( j = 1 \) to \( n \):

$$
\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} (f^{(i)} - y^{(i)}) x_j^{(i)} + \frac{\lambda}{m} w_j
$$

- For \( b \) (no regularization):

$$
\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (f^{(i)} - y^{(i)})
$$

### Gradient Descent Algorithm:

```python
# Regularized Logistic Regression (gradient descent)
for j in range(n):
    w[j] = w[j] - alpha * ( (1/m) * sum((f[i] - y[i]) * x[i][j] for i in range(m)) + (lambda_/m) * w[j] )

b = b - alpha * (1/m) * sum(f[i] - y[i] for i in range(m))
```

---

## âœ… Result

Regularization prevents large weights \( w_j \), reducing model complexity, and avoids overfitting â€” especially useful for logistic regression with many features.

---

## ğŸ§ª Tip

Try this in the lab:

- Enable regularization.
- Adjust \( \lambda \) and observe decision boundaries.

---

## ğŸš€ Coming Next

In the next course, you'll build **neural networks**, leveraging what you've learned (cost functions, gradient descent, sigmoid) to solve more complex tasks like:

- Image recognition
- Self-driving cars
- Speech processing

---

ğŸ‰ **Great job finishing Course 1!**
