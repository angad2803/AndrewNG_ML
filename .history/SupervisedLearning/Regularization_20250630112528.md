# ğŸ“˜ Regularization in Machine Learning â€” Summary

Regularization is a technique used to reduce **overfitting** in machine learning models by penalizing large weights.

---

## ğŸ”‘ Key Concepts

### 1. **Problem of Overfitting**

- High-degree polynomials (e.g. xÂ³, xâ´) can fit training data too well (wiggly curves).
- These models perform poorly on new/unseen data (poor generalization).

---

### 2. **Solution: Regularization**

- Shrinks large weight values **wâ‚ to wâ‚™** toward zero.
- Simpler models generalize better.
- Keeps all features, unlike feature selection.

---

### 3. **Modified Cost Function**

#### Original (Linear Regression):

\[
J(w, b) = \frac{1}{2m} \sum (f(x^{(i)}) - y^{(i)})^2
\]

#### Regularized:

\[
J*{reg}(w, b) = \frac{1}{2m} \sum (f(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m} \sum*{j=1}^{n} w_j^2
\]

- The second term is the **regularization term**.
- \(\lambda\) (lambda) is the **regularization parameter**.

---

### 4. **Effect of Lambda (Î»)**

| Î» Value   | Effect                                     |
| --------- | ------------------------------------------ |
| Î» = 0     | No regularization â†’ Overfitting            |
| Î» â†’ âˆ     | Very strong regularization â†’ Underfitting  |
| Optimal Î» | Balance between underfitting & overfitting |

---

### 5. **Practical Notes**

- Usually **do not regularize** the bias term (b).
- Regularization is commonly used in **linear**, **logistic**, and **neural networks**.
- Helps maintain all features, but limits their influence.

---

## âœ… Summary

Regularization helps prevent overfitting by penalizing large weights through an extra term in the cost function. Choosing a good Î» is key to balancing model complexity and generalization.

# ğŸ§  Regularized Linear Regression: Gradient Descent

## ğŸ“˜ Cost Function (with Regularization)

```
J(w, b) = (1/2m) * Î£ [f(xá¶¦) - yá¶¦]Â² + (Î»/2m) * Î£ [wâ±¼Â²]
```

- First term: Squared error (fit to data)
- Second term: Regularization (shrinks parameters)
- Î» (lambda): Regularization parameter
- m: Number of training examples

---

## ğŸ” Gradient Descent Updates (Regularized)

### For wâ±¼ (j = 1 to n)

```
wâ±¼ := wâ±¼ - Î± * [ (1/m) * Î£(f(xá¶¦) - yá¶¦) * xâ±¼á¶¦ + (Î»/m) * wâ±¼ ]
```

### For b (not regularized)

```
b := b - Î± * (1/m) * Î£(f(xá¶¦) - yá¶¦)
```

---

## ğŸ” Interpretation

- Regularization adds `+ (Î»/m) * wâ±¼` to the gradient.
- Update rule becomes:

```
wâ±¼ := wâ±¼ * (1 - Î±Î»/m) - Î± * usual_gradient
```

â†’ This shrinks `wâ±¼` slightly every iteration.

---

## ğŸ’¡ Why It Works

- Penalizing large weights reduces overfitting.
- Keeps model simpler â†’ better generalization.
- Î» too large â†’ underfitting  
  Î» = 0 â†’ overfitting  
  Choose **just right** via validation.

---

## ğŸ§® Optional Derivative Breakdown

```
âˆ‚J/âˆ‚wâ±¼ = (1/m) * Î£[(f(xá¶¦) - yá¶¦) * xâ±¼á¶¦] + (Î»/m) * wâ±¼
```

---

## âœ… Summary

- Regularization shrinks `wâ±¼` on each iteration.
- Prevents overfitting in models with many features.
- Small change to gradient descent â†’ big improvement.
