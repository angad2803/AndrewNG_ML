# üìò Regularization in Machine Learning ‚Äî Summary

Regularization is a technique used to reduce **overfitting** in machine learning models by penalizing large weights.

---

## üîë Key Concepts

### 1. **Problem of Overfitting**

- High-degree polynomials (e.g. x¬≥, x‚Å¥) can fit training data too well (wiggly curves).
- These models perform poorly on new/unseen data (poor generalization).

---

### 2. **Solution: Regularization**

- Shrinks large weight values **w‚ÇÅ to w‚Çô** toward zero.
- Simpler models generalize better.
- Keeps all features, unlike feature selection.

---

### 3. **Modified Cost Function**

#### Original (Linear Regression):

\[
J(w, b) = \frac{1}{2m} \sum (f(x^{(i)}) - y^{(i)})^2
\]

#### Regularized:

\[
J*{reg}(w, b) = \frac{1}{2m} \sum (f(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m} \sum*{j=1}^{n} w_j^2
\]

- The second term is the **regularization term**.
- \(\lambda\) (lambda) is the **regularization parameter**.

---

### 4. **Effect of Lambda (Œª)**

| Œª Value   | Effect                                     |
| --------- | ------------------------------------------ |
| Œª = 0     | No regularization ‚Üí Overfitting            |
| Œª ‚Üí ‚àû     | Very strong regularization ‚Üí Underfitting  |
| Optimal Œª | Balance between underfitting & overfitting |

---

### 5. **Practical Notes**

- Usually **do not regularize** the bias term (b).
- Regularization is commonly used in **linear**, **logistic**, and **neural networks**.
- Helps maintain all features, but limits their influence.

---

## ‚úÖ Summary

Regularization helps prevent overfitting by penalizing large weights through an extra term in the cost function. Choosing a good Œª is key to balancing model complexity and generalization.
