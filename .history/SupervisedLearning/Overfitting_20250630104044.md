## ðŸ“‰ Underfitting vs. Overfitting in Machine Learning

---

### ðŸŽ¯ Objective

Understand **underfitting**, **overfitting**, and how they affect your model's **generalization** to unseen data.

---

### ðŸ§  Key Concepts

| Concept          | Description                                                                   |
| ---------------- | ----------------------------------------------------------------------------- |
| **Underfitting** | Model is too simple to capture patterns in training data                      |
| **Overfitting**  | Model is too complex and fits the training data too well, including noise     |
| **Just Right**   | Model fits the training data well and generalizes to unseen examples          |
| **Bias**         | Error due to overly simplistic assumptions (underfitting)                     |
| **Variance**     | Error due to sensitivity to small fluctuations in training data (overfitting) |

---

### ðŸ  Housing Price Prediction Example

#### ðŸ“‰ Underfitting (High Bias)

- Linear model fits a straight line through data
- Misses non-linear patterns
- Model has high training error

#### âœ… Just Right

- Quadratic model captures the trend in housing prices better
- Fits training data fairly well
- Likely to generalize well on new examples

#### ðŸ“ˆ Overfitting (High Variance)

- Fourth-order polynomial fits training data **perfectly**
- Very "wiggly" curve with poor real-world logic
- Will generalize **poorly** to new unseen data

---

### âš–ï¸ Bias vs. Variance

|                | High Bias (Underfitting)    | High Variance (Overfitting)    |
| -------------- | --------------------------- | ------------------------------ |
| Model          | Too simple                  | Too complex                    |
| Training Error | High                        | Low                            |
| Test Error     | High                        | High                           |
| Sensitivity    | Insensitive to data changes | Very sensitive to data changes |
| Generalization | Poor                        | Poor                           |

---

### ðŸ§ª Classification Example: Tumor Detection

#### Underfitting (High Bias)

- Simple linear decision boundary using logistic regression
- Doesn't classify well; misses data patterns

#### Just Right

- Use quadratic features (like \( x_1^2 \), \( x_2^2 \), etc.)
- Elliptical boundary separates data well
- Classifies accurately and generalizes well

#### Overfitting (High Variance)

- Complex, high-order polynomial decision boundary
- Twists to perfectly classify all training points
- Will fail on slightly different/new data

---

### ðŸ§  Mental Model: Goldilocks Analogy

- **Too simple** (underfitting) â†’ â„ï¸ Cold porridge (not useful)
- **Too complex** (overfitting) â†’ ðŸ”¥ Hot porridge (not useful)
- **Just right** â†’ ðŸŒ¡ï¸ Porridge thatâ€™s perfect to eat (generalizes well)

---

### âœ… Summary

- **Underfitting / High Bias**: Model can't even fit training data properly.
- **Overfitting / High Variance**: Model fits training data too well, poor generalization.
- **Goal**: Choose a model that's **just right** in complexity.

> ðŸŽ¯ In the next video, you'll learn how to fix overfitting using **regularization**, a powerful and commonly used technique.
