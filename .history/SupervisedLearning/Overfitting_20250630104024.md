## 📉 Underfitting vs. Overfitting in Machine Learning

---

### 🎯 Objective

Understand **underfitting**, **overfitting**, and how they affect your model's **generalization** to unseen data.

---

### 🧠 Key Concepts

| Concept          | Description                                                                   |
| ---------------- | ----------------------------------------------------------------------------- |
| **Underfitting** | Model is too simple to capture patterns in training data                      |
| **Overfitting**  | Model is too complex and fits the training data too well, including noise     |
| **Just Right**   | Model fits the training data well and generalizes to unseen examples          |
| **Bias**         | Error due to overly simplistic assumptions (underfitting)                     |
| **Variance**     | Error due to sensitivity to small fluctuations in training data (overfitting) |

---

### 🏠 Housing Price Prediction Example

#### 📉 Underfitting (High Bias)

- Linear model fits a straight line through data
- Misses non-linear patterns
- Model has high training error

#### ✅ Just Right

- Quadratic model captures the trend in housing prices better
- Fits training data fairly well
- Likely to generalize well on new examples

#### 📈 Overfitting (High Variance)

- Fourth-order polynomial fits training data **perfectly**
- Very "wiggly" curve with poor real-world logic
- Will generalize **poorly** to new unseen data

---

### ⚖️ Bias vs. Variance

|                | High Bias (Underfitting)    | High Variance (Overfitting)    |
| -------------- | --------------------------- | ------------------------------ |
| Model          | Too simple                  | Too complex                    |
| Training Error | High                        | Low                            |
| Test Error     | High                        | High                           |
| Sensitivity    | Insensitive to data changes | Very sensitive to data changes |
| Generalization | Poor                        | Poor                           |

---

### 🧪 Classification Example: Tumor Detection

#### Underfitting (High Bias)

- Simple linear decision boundary using logistic regression
- Doesn't classify well; misses data patterns

#### Just Right

- Use quadratic features (like \( x_1^2 \), \( x_2^2 \), etc.)
- Elliptical boundary separates data well
- Classifies accurately and generalizes well

#### Overfitting (High Variance)

- Complex, high-order polynomial decision boundary
- Twists to perfectly classify all training points
- Will fail on slightly different/new data

---

### 🧠 Mental Model: Goldilocks Analogy

- **Too simple** (underfitting) → ❄️ Cold porridge (not useful)
- **Too complex** (overfitting) → 🔥 Hot porridge (not useful)
- **Just right** → 🌡️ Porridge that’s perfect to eat (generalizes well)

---

### ✅ Summary

- **Underfitting / High Bias**: Model can't even fit training data properly.
- **Overfitting / High Variance**: Model fits training data too well, poor generalization.
- **Goal**: Choose a model that's **just right** in complexity.

> 🎯 In the next video, you'll learn how to fix overfitting using **regularization**, a powerful and commonly used technique.
