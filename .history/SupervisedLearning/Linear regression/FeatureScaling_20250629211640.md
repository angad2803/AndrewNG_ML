# Feature Scaling in Gradient Descent

## Why Feature Scaling Matters

In gradient descent, features with **very different ranges** can cause the cost function to form **elongated contours**, leading to **slow convergence**.

### Example

Consider predicting house price using:

- **x₁**: Size in square feet (range: 300–2000)
- **x₂**: Number of bedrooms (range: 0–5)

Two scenarios:

1. `w₁ = 50`, `w₂ = 0.1`, `b = 50` → Prediction is far from actual price.
2. `w₁ = 0.1`, `w₂ = 50`, `b = 50` → Prediction is accurate.

**Insight**:

- Large-range features → smaller weights
- Small-range features → larger weights

## Problem with Gradient Descent

Plotting the cost function using unscaled features leads to:

- Contour plots shaped like **narrow ellipses**
- Gradient descent **zigzags** and takes many steps to converge

This is because:

- Small changes in `w₁` (associated with large-range `x₁`) change predictions drastically
- Large changes in `w₂` (associated with small-range `x₂`) barely affect predictions

## Solution: Feature Scaling

Rescale features so they have **similar ranges**, for example:

- \( x_1 \in [0, 1] \)
- \( x_2 \in [0, 1] \)

Benefits:

- Contours of cost function become **more circular**
- Gradient descent can take **more direct steps** to the global minimum
- **Faster and more stable convergence**

## Summary

- **Unscaled features** lead to inefficient, slow gradient descent.
- **Feature scaling** transforms the feature space so all features contribute proportionately.
- This helps gradient descent **converge faster and more reliably**.

➡️ In the next step, you'll learn how to implement feature scaling in code.
