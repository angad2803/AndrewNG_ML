# Training Neural Networks

## Activation Functions

![alt text](image-54.png)

Then there are various other RELU's as mentioned in michigan slides but mostly all of them have similar accuracy on the cifar 10

![alt text](image-55.png)

Don‚Äôt think too hard. Just use ReLU

## Data Preprocessing

![alt text](image-56.png)

## Weight Initialization

![alt text](image-57.png)

## Regularization

![alt text](image-59.png)

1. Dropout- In each forward pass, randomly set some neurons to zero Probability of dropping is a hyperparameter; 0.5 is common

![alt text](image-58.png)

Forces the network to have a redundant representation; Prevents co-adaptation of features

At test time, drop nothing and multiply by dropout probability

Later architectures (GoogLeNet,ResNet, etc) use global average pooling instead of fully-connected layers: they don‚Äôt use dropout at all!

## Data Augmentation: Get creative for your problem!

![alt text](image-60.png)

## Learning rate

![alt text](image-61.png)

![alt text](image-62.png)

How long to train? Early Stopping

Stop training the model when accuracy on the validation set decreases
Or train for a long time, but always keep track of the model snapshot that
worked best on val. Always a good idea to do this!

## Choosing Hyperparameters

There is a lot of info on this , Grid search vs Random Search but CampusX gave Bayesian search in which we used Optuna so idk if i need to cover grid and random when Bayesian Search is better

## Transfer Learning (Very important)

Definition: Instead of training a CNN from scratch (which needs huge data + compute), we ‚Äútransfer‚Äù the knowledge learned by a large pre-trained model (like ResNet, VGG, EfficientNet, etc.) to a new task.

Analogy: Imagine you learned to play cricket. Now when learning baseball, you don‚Äôt start from zero‚Äîyou already know how to swing, catch, etc. That prior knowledge helps you adapt faster.

### Why Transfer Learning in CNNs?

CNNs trained on huge datasets (like ImageNet with 1.2M images, 1000 classes) learn:

Early layers ‚Üí detect simple patterns (edges, corners, textures).

Middle layers ‚Üí detect shapes (circles, patterns, object parts).

Later layers ‚Üí detect task-specific concepts (dogs, cars, faces).

üëâ These early and middle features are general, so we can reuse them for new tasks with smaller datasets.

![alt text](image-63.png)

Higher Accuracy: Models like ResNet have seen millions of images‚Äîthis prior knowledge helps generalize.

![alt text](image-64.png)

Pretraining + Finetuning beats training from scratch when dataset size is very small
